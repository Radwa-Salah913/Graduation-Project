{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# import tensorflow\n",
    "import cv2\n",
    "import time\n",
    "import os\n",
    "import mediapipe as mp\n",
    "from numpy import expand_dims , where ,argmax\n",
    "# from tensorflow.keras.preprocessing.image import  img_to_array\n",
    "\n",
    "\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "from ultralytics import YOLO\n",
    "from ultralytics.utils.plotting import Annotator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_cheating(ROI):\n",
    "    ROI.flags.writeable = False\n",
    "    posingEstimation = face_mesh.process(ROI)\n",
    "    ROI.flags.writeable = True\n",
    "#////////////////////////////////////////////////////////////////////////////////////////\n",
    "    rows, cols, _ =ROI.shape\n",
    "    face_3d = []\n",
    "    face_2d = []\n",
    "#///////////////////////////////////////////////////////////////////////////////////\n",
    "    if posingEstimation.multi_face_landmarks:\n",
    "        for facelms in posingEstimation.multi_face_landmarks:\n",
    "#//////////////////////////////////////////////////////////////////////////////\n",
    "            for id ,lm in enumerate(facelms.landmark):\n",
    "                a,b=int(lm.x*cols),int(lm.y*rows)\n",
    "                if id in [1] :\n",
    "                    #cv2.circle(frame,(a,b),10,(0,0,255),-1)\n",
    "\n",
    "                    nose_2d=(a , b)\n",
    "                    nose_3d=(a , b , lm.z * 3000)\n",
    "\n",
    "                x , y = int(a), int(b)\n",
    "                face_2d.append([x,y])\n",
    "                face_3d.append([x,y,lm.z])\n",
    "            face_2d = np.array(face_2d, dtype=np.float64)\n",
    "            face_3d = np.array(face_3d, dtype=np.float64)\n",
    "#////////////////////////////////////////////////////////////////////////////////////\n",
    "            focal_len = 1* c\n",
    "            cam_matrix = np.array([ [focal_len , 0 , r/2],\n",
    "                                    [0 , focal_len , c/2],\n",
    "                                    [0,0,1]])\n",
    "            dist_matrix= np.zeros((4,1), dtype=np.float64)\n",
    "#//////////////////////////////////////////////////////////////////////////////////////////////\n",
    "            success , rot_vec , trans_vec =cv2.solvePnP(face_3d, face_2d , cam_matrix , dist_matrix)\n",
    "            rmat , jac= cv2.Rodrigues(rot_vec)\n",
    "            angles , mtxR , mtxQ , Qx , Qy , Qz= cv2.RQDecomp3x3(rmat)\n",
    "            x=angles[0] * 360\n",
    "            y=angles[1] * 360\n",
    "            z=angles[2] * 360\n",
    "\n",
    "#//////////////////////////////////////////////////////////////////////////////////////////\n",
    "           \n",
    "            if x < -40:     # head tilted to the left\n",
    "                text = \"Cheating\"\n",
    "            elif x > 15:     # head tilted to the right\n",
    "                text = \"Cheating\"\n",
    "            elif y < -40:      # looking down\n",
    "                text = \"Cheating\"\n",
    "            elif y > 40:     # looking up\n",
    "                text = \"Normal\"\n",
    "            else:            \n",
    "                text = \"Normal\"\n",
    "\n",
    "        return text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_id = {}\n",
    "time_temp = {}\n",
    "student_id={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_track(text, track_id):\n",
    "    if text == \"Cheating\":\n",
    "        if track_id not in time_temp:\n",
    "            # If the track_id doesn't exist, add it with the current time in milliseconds\n",
    "            time_temp[track_id] = [int(time.time()) , True]\n",
    "\n",
    "    elif text == \"Normal\":\n",
    "        # Calculate the difference between the current time and the previous cheating time\n",
    "        if track_id in time_temp and time_temp[track_id][1]:\n",
    "            current_time = int(time.time())\n",
    "            time_difference = current_time - time_temp[track_id][0]\n",
    "            # Update the time for the track_id\n",
    "            if track_id in time_id:\n",
    "                time_id[track_id] += time_difference\n",
    "            else:\n",
    "                time_id[track_id] = time_difference\n",
    "            time_temp.pop(track_id)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(img):\n",
    "    img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
    "    img=cv2.resize(img,dsize=(224,224),interpolation=cv2.INTER_CUBIC)\n",
    "    img = img_to_array(img)\n",
    "    img = img/255.0\n",
    "    img = expand_dims(img, axis=0)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO(r\"C:\\\\Users\\\\DELL\\\\Downloads\\\\YoloV8 Face.pt\")\n",
    "names = model.model.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mobile_detection_model = YOLO(r\"c:\\\\Users\\\\dell\\\\OneDrive\\\\Desktop\\\\Graduation project\\\\best best (mobile) .pt\")\n",
    "mobile_detection_names = mobile_detection_model.model.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mpFacemesh = mp.solutions.face_mesh\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "face_mesh = mpFacemesh.FaceMesh(refine_landmarks=True, min_detection_confidence=0.5, min_tracking_confidence=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mobile=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# face recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import ZeroPadding2D, Convolution2D, \\\n",
    "MaxPooling2D, Dropout, Flatten, Activation\n",
    "from tensorflow.keras.models import Sequential ,load_model\n",
    "from tensorflow.keras.models import Model\n",
    "def vgg_face():\n",
    "    model = Sequential()\n",
    "    model.add(ZeroPadding2D((1,1),input_shape=(224,224, 3)))\n",
    "    model.add(Convolution2D(64, (3, 3), activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(64, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "    \n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(128, (3, 3), activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(128, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "    \n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(256, (3, 3), activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(256, (3, 3), activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(256, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "    \n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "    \n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "    \n",
    "    model.add(Convolution2D(4096, (7, 7), activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Convolution2D(4096, (1, 1), activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Convolution2D(2622, (1, 1)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Activation('softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anconda\\envs\\mediapipe_env\\lib\\site-packages\\keras\\src\\layers\\reshaping\\zero_padding2d.py:72: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "model_vgg = vgg_face()\n",
    "model_vgg = Model(inputs=model_vgg.layers[0].input, outputs=model_vgg.layers[-2].output)\n",
    "model_vgg.load_weights('c:\\\\Users\\\\dell\\\\OneDrive\\\\Desktop\\\\Graduation project\\\\vgg_face_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import  img_to_array\n",
    "\n",
    "\n",
    "# Preprocessing of image \n",
    "def preprocess_image(img):\n",
    "    img = img_to_array(img)\n",
    "    img = img/255.0\n",
    "    img = expand_dims(img, axis=0)\n",
    "    return img\n",
    "\n",
    "# to identify the student\n",
    "def Face_Recognition(roi):\n",
    "    roi = cv2.cvtColor(roi,cv2.COLOR_BGR2RGB)\n",
    "    roi=cv2.resize(roi,dsize=(224,224),interpolation=cv2.INTER_CUBIC)\n",
    "    roi=preprocess_image(roi)\n",
    "    embedding_vector = model_vgg.predict(roi)[0]\n",
    "    return embedding_vector\n",
    "\n",
    "\n",
    "def findCosineSimilarity(source_representation, test_representation):\n",
    "    a = np.matmul(np.transpose(source_representation), test_representation)\n",
    "    b = np.sum(np.multiply(source_representation, source_representation))\n",
    "    c = np.sum(np.multiply(test_representation, test_representation))\n",
    "    return 1 - (a / (np.sqrt(b) * np.sqrt(c)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create an empty DataFrame with 2623 columns\n",
    "columns = [f\"Column_{i}\" for i in range(1, 2624)]\n",
    "points_of_face = pd.DataFrame(columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_id_temp={}\n",
    "student_id={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 (no detections), 253.4ms\n",
      "Speed: 4.3ms preprocess, 253.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 752ms/step\n",
      "\n",
      "0: 480x640 (no detections), 293.1ms\n",
      "Speed: 6.4ms preprocess, 293.1ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 415ms/step\n",
      "\n",
      "0: 480x640 (no detections), 243.7ms\n",
      "Speed: 3.3ms preprocess, 243.7ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 356ms/step\n",
      "\n",
      "0: 480x640 (no detections), 242.8ms\n",
      "Speed: 2.0ms preprocess, 242.8ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 341ms/step\n",
      "\n",
      "0: 480x640 (no detections), 234.5ms\n",
      "Speed: 2.0ms preprocess, 234.5ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 406ms/step\n",
      "\n",
      "0: 480x640 (no detections), 217.2ms\n",
      "Speed: 2.0ms preprocess, 217.2ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 336ms/step\n",
      "\n",
      "0: 480x640 (no detections), 218.6ms\n",
      "Speed: 2.1ms preprocess, 218.6ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 212.3ms\n",
      "Speed: 1.3ms preprocess, 212.3ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 377ms/step\n",
      "\n",
      "0: 480x640 (no detections), 245.7ms\n",
      "Speed: 7.9ms preprocess, 245.7ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 373ms/step\n",
      "\n",
      "0: 480x640 (no detections), 252.4ms\n",
      "Speed: 7.8ms preprocess, 252.4ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 378ms/step\n",
      "\n",
      "0: 480x640 (no detections), 230.2ms\n",
      "Speed: 1.5ms preprocess, 230.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 421ms/step\n",
      "\n",
      "0: 480x640 (no detections), 249.9ms\n",
      "Speed: 3.0ms preprocess, 249.9ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 398ms/step\n",
      "\n",
      "0: 480x640 (no detections), 237.3ms\n",
      "Speed: 3.9ms preprocess, 237.3ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 387ms/step\n"
     ]
    }
   ],
   "source": [
    "# Load video\n",
    "cap = cv2.VideoCapture(0) # \"video.mp4\"\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# Set video codec and frame rate\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "out = cv2.VideoWriter('outputzz11.avi', fourcc, 30, (width, height))  # Adjust frame rate (30 fps in this example)\n",
    "\n",
    "session_start=int(time.time())\n",
    "\n",
    "while cap.isOpened():\n",
    "    success, frame = cap.read()\n",
    "    if not success or (cv2.waitKey(1) & 0xFF == ord(\"q\")):\n",
    "        break\n",
    "    r, c, _ = frame.shape\n",
    "    annotator = Annotator(frame, line_width=2)\n",
    "\n",
    "    face_recognized=set() # reset the set which contains faces that appear with each frame\n",
    "    removed_ids=[]\n",
    "   ########################################################################################################################\n",
    "   # Mobile_detection_Tracking\n",
    "\n",
    "    result1=mobile_detection_model(frame)\n",
    "\n",
    "    for result in result1[0].boxes.cpu().numpy():\n",
    "            x1,y1,x2,y2=map(int,result.xyxy[0])\n",
    "            cls=names[int(result.cls[0])]\n",
    "            conf=result.conf[0].round(2)\n",
    "           \n",
    "            if conf >0.90 :\n",
    "                mobile=True\n",
    "   ################################################################################################################################\n",
    "   # Cheating_detection_tracking\n",
    "\n",
    "    results = model.track(frame, iou=0.6, show=False, tracker=\"bytetrack.yaml\", persist=True, verbose=False)\n",
    "\n",
    "    if results[0].boxes.id is not None:\n",
    "        track_ids = results[0].boxes.id.int().cpu().tolist()\n",
    "        cls = names[results[0].boxes.cls.int().cpu().tolist()[0]]\n",
    "        boxes = results[0].boxes.xyxy.cpu()\n",
    "        conf = results[0].boxes.conf.tolist() \n",
    "        for box, track_id, cof, cc in zip(boxes, track_ids, conf, cls):\n",
    "            x1, y1, x2, y2 = box.int().tolist()   \n",
    "            ROI = frame[y1:y2, x1:x2]\n",
    "            if ROI.size != 0 :\n",
    "                ROI = cv2.cvtColor(ROI,cv2.COLOR_BGR2RGB)\n",
    "                text = check_cheating(ROI)\n",
    "\n",
    "                emded=Face_Recognition(ROI)\n",
    "                flag=False # define the flag to determine if the person appears for the first time or not\n",
    "                id=track_id \n",
    "                for i in range(points_of_face.shape[0]):\n",
    "                    sim=findCosineSimilarity(emded,points_of_face.iloc[i,:-1])\n",
    "                    if sim > 0.2 :\n",
    "                        id=points_of_face.iloc[i,-1] # if person appear in the past -----> doesn't take new id\n",
    "                        flag=True\n",
    "                    \n",
    "                        \n",
    "                        \n",
    "                face_recognized.add(id)   \n",
    "                if flag == False:\n",
    "                    x=[] # list to collect vector of face_recognition (emded) and id \n",
    "                    for value in emded:\n",
    "                        x.append(value) \n",
    "                    x.append(track_id)\n",
    "                    \n",
    "                    points_of_face.loc[len(points_of_face)] = (x)   # add new person to points_of_face dataframe\n",
    "################################################################################################################################\n",
    "                if id not in time_id:\n",
    "                    time_id[id] = 0\n",
    "                process_track(text, id)\n",
    "################################################################################\n",
    "                # calculate time for each person appear\n",
    "\n",
    "                for i in face_recognized:\n",
    "                    if i in student_id_temp.keys(): # if the person appears in the  frame (appears in face_recognized list) and he existed in previous frames ---> then he continues ---> then update last_seen_time only\n",
    "                            student_id_temp[i]['last_seen_time']=int(time.time())\n",
    "                    else: # he appears in the  frame (appears in face_recognized list) and he wasn't exist in previous frames -----> then he appears for the first time\n",
    "                        student_id_temp[i]={\n",
    "                            'first_seen_time': int(time.time()),\n",
    "                            'last_seen_time': int(time.time())\n",
    "                        }   \n",
    "\n",
    "\n",
    "                for j in student_id_temp.keys():  \n",
    "                    if j not in face_recognized:# if the person exists in (student_id_temp) and not exists in (face_recognized) ----> then he is leave session ----> then I can calculate the duration that he appeared in it\n",
    "                        if j not in student_id:\n",
    "                            student_id[j] = (student_id_temp[j]['last_seen_time']-student_id_temp[j]['first_seen_time'])\n",
    "                        else:\n",
    "                            student_id[j] += (student_id_temp[j]['last_seen_time']-student_id_temp[j]['first_seen_time'])   \n",
    "\n",
    "                        removed_ids.append(j) \n",
    "\n",
    "\n",
    "                for id in removed_ids: # remove the person that leaves from (student_id_temp) ------> bec if he appears again calculate time correctly by resetting (first_seen_time)\n",
    "                    student_id_temp.pop(id,None)      \n",
    "               \n",
    "            \n",
    "################################################################################################################################################\n",
    "    # Write frame to output video\n",
    "    out.write(frame)\n",
    "    # Display frame\n",
    "    cv2.imshow('img', frame)\n",
    "\n",
    "\n",
    "# Release video capture and writer\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "session_end=int(time.time())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the time for people who exist until the end\n",
    "for j in student_id_temp.keys():\n",
    "    if j not in student_id:\n",
    "        student_id[j] = (student_id_temp[j]['last_seen_time']-student_id_temp[j]['first_seen_time'])\n",
    "    else:\n",
    "        student_id[j] += (student_id_temp[j]['last_seen_time']-student_id_temp[j]['first_seen_time']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 9}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 5}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_id # contain -----> {id of student , time who is take turns his face}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_duration=0\n",
    "for key,value in student_id.items():\n",
    "    if value >= student_duration:\n",
    "        student_duration=value\n",
    "        id_of_real_student=key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_time_of_session = session_end - session_start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determine if the student is a cheater or polite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_score=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "polite\n"
     ]
    }
   ],
   "source": [
    "result='polite'\n",
    "\n",
    "if (len(student_id)>1 ): # if more than one person is appearing in the frame and it takes a time of more than 3 seconds\n",
    "    result=\"Cheater\"\n",
    "    total_score-=20\n",
    "if time_id[id_of_real_student] > 15:  # if a student turns around and takes time more than 15 seconds \n",
    "    result=\"Cheater\"\n",
    "    total_score-=10\n",
    "if mobile: # if the student holds the mobile phone\n",
    "    result=\"Cheater\"\n",
    "    total_score-=30\n",
    "\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "if total_time_of_session - student_duration > 10: # If the student closes (covers) the camera or leavea the exam\n",
    "    total_score=0                                       # Then he cheated all the session ------> lost all the score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage_of_cheating=(100-total_score)/100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "polite 0.0\n"
     ]
    }
   ],
   "source": [
    "print(result,percentage_of_cheating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "percentage_of_cheating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if the student holds the mobile phone or not\n",
    "mobile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how many time a student turns around \n",
    "time_id[id_of_real_student]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how many time the student doesn't attend the session or try to covers the camera\n",
    "(total_time_of_session - student_duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# how many another people exist during the session and how many time they attend\n",
    "count_of_another_people=0\n",
    "time_of_another_people=0\n",
    "\n",
    "\n",
    "for i,j in student_id.items():\n",
    "    if( i != id_of_real_student):\n",
    "       count_of_another_people +=1\n",
    "       time_of_another_people += j\n",
    "\n",
    "\n",
    "print(count_of_another_people)\n",
    "print(time_of_another_people)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try to open another tab or take a screenshots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mediapipe_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
