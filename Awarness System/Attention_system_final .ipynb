{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow\n",
    "import cv2\n",
    "import time\n",
    "import os\n",
    "import math\n",
    "import mediapipe as mp\n",
    "from numpy import expand_dims , where ,argmax\n",
    "\n",
    "\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "from ultralytics import YOLO\n",
    "from ultralytics.utils.plotting import Annotator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_model = YOLO(r'c:/Users/dell/OneDrive/Desktop/Graduation project/yolov8n.pt') \n",
    "mobile_model = YOLO(r\"c:/Users/dell/OneDrive/Desktop/Graduation project/best best (mobile) .pt\")\n",
    "names = mobile_model.model.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define class index for 'person' (usually 0 in COCO dataset)\n",
    "PERSON_CLASS_INDEX = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize video capture\n",
    "cap = cv2.VideoCapture(0)  # Replace with your video path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frame count for tracking disappeared persons\n",
    "time_disappear_limit = 10  # Number of frames to consider a person as exited # لو الشخص مختفي لمدة 30 فريم، نعتبره مشي"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "people_who_exist={}\n",
    "session_start=0\n",
    "session_end=0\n",
    "exist_all_session=False\n",
    "another_people=False\n",
    "mobile=False\n",
    "time_id = {}\n",
    "time_temp = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eye State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize MediaPipe Face Mesh\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "# Constants\n",
    "EYE_CLOSED_THRESHOLD = 0.35\n",
    "\n",
    "# Function to calculate EAR (Eye Aspect Ratio)\n",
    "def calculate_ear(eye_landmarks):\n",
    "    A = math.dist(eye_landmarks[1], eye_landmarks[5])\n",
    "    B = math.dist(eye_landmarks[2], eye_landmarks[4])\n",
    "    C = math.dist(eye_landmarks[0], eye_landmarks[3])\n",
    "    ear = (A + B) / (2.0 * C)\n",
    "    return ear\n",
    "\n",
    "def detect_eyes_state(ROI):\n",
    "   \n",
    "    rows, cols, _ = ROI.shape\n",
    "    posingEstimation = face_mesh.process(ROI)\n",
    "\n",
    "    if posingEstimation.multi_face_landmarks:\n",
    "        for facelms in posingEstimation.multi_face_landmarks:\n",
    "            # Get key landmarks for gaze detection\n",
    "            nose = facelms.landmark[1]  # Nose tip\n",
    "            left_eye_outer = facelms.landmark[33]  # Left eye outer corner\n",
    "            right_eye_outer = facelms.landmark[263]  # Right eye outer corner\n",
    "\n",
    "            # Get landmarks for eye EAR calculation\n",
    "            left_eye_landmarks = [\n",
    "                facelms.landmark[i] for i in [33, 159, 158, 133, 153, 144]\n",
    "            ]\n",
    "            right_eye_landmarks = [\n",
    "                facelms.landmark[i] for i in [362, 386, 385, 263, 373, 380]\n",
    "            ]\n",
    "\n",
    "           \n",
    "\n",
    "            # Calculate EAR for both eyes\n",
    "            left_eye_ear = calculate_ear(\n",
    "                [(lm.x * cols, lm.y * rows) for lm in left_eye_landmarks]\n",
    "            )\n",
    "            right_eye_ear = calculate_ear(\n",
    "                [(lm.x * cols, lm.y * rows) for lm in right_eye_landmarks]\n",
    "            )\n",
    "\n",
    "            # Determine if eyes are open or closed\n",
    "            if left_eye_ear < EYE_CLOSED_THRESHOLD and right_eye_ear < EYE_CLOSED_THRESHOLD:\n",
    "                eye_state = \"Closed\"\n",
    "            else:\n",
    "                eye_state = \"Open\"\n",
    "\n",
    "            return  eye_state\n",
    "\n",
    "    return \"No Face Detected\", \"Unknown\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the time when the student closes his eye\n",
    "\n",
    "def process_track(text, track_id):  \n",
    "    if text == \"Closed\":\n",
    "        if track_id not in time_temp:\n",
    "            # If the track_id doesn't exist ------> add it with the current time \n",
    "            time_temp[track_id] = [int(time.time()) , True]\n",
    "\n",
    "    elif text == \"Open\":\n",
    "        # Calculate the difference between the current time and the previous eye_state\n",
    "        if track_id in time_temp and time_temp[track_id][1]:\n",
    "            current_time = int(time.time())\n",
    "            time_difference = current_time - time_temp[track_id][0]\n",
    "            # Update the time for the track_id\n",
    "            if track_id in time_id:\n",
    "                time_id[track_id] = max( time_difference , time_id[track_id])\n",
    "            else:\n",
    "                time_id[track_id] = time_difference\n",
    "            time_temp.pop(track_id)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import ZeroPadding2D, Convolution2D, \\\n",
    "MaxPooling2D, Dropout, Flatten, Activation\n",
    "from tensorflow.keras.models import Sequential ,load_model\n",
    "from tensorflow.keras.models import Model\n",
    "def vgg_face():\n",
    "    model = Sequential()\n",
    "    model.add(ZeroPadding2D((1,1),input_shape=(224,224, 3)))\n",
    "    model.add(Convolution2D(64, (3, 3), activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(64, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "    \n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(128, (3, 3), activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(128, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "    \n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(256, (3, 3), activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(256, (3, 3), activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(256, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "    \n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "    \n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "    \n",
    "    model.add(Convolution2D(4096, (7, 7), activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Convolution2D(4096, (1, 1), activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Convolution2D(2622, (1, 1)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Activation('softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anconda\\envs\\mediapipe_env\\lib\\site-packages\\keras\\src\\layers\\reshaping\\zero_padding2d.py:72: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "model_vgg = vgg_face()\n",
    "model_vgg = Model(inputs=model_vgg.layers[0].input, outputs=model_vgg.layers[-2].output)\n",
    "model_vgg.load_weights('c:\\\\Users\\\\dell\\\\OneDrive\\\\Desktop\\\\Graduation project\\\\vgg_face_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import  img_to_array\n",
    "\n",
    "\n",
    "# Preprocessing of image \n",
    "def preprocess_image(img):\n",
    "    img = img_to_array(img)\n",
    "    img = img/255.0\n",
    "    img = expand_dims(img, axis=0)\n",
    "    return img\n",
    "\n",
    "# to identify the student\n",
    "def Face_Recognition(roi):\n",
    "    roi = cv2.cvtColor(roi,cv2.COLOR_BGR2RGB)\n",
    "    roi=cv2.resize(roi,dsize=(224,224),interpolation=cv2.INTER_CUBIC)\n",
    "    roi=preprocess_image(roi)\n",
    "    embedding_vector = model_vgg.predict(roi)[0]\n",
    "    return embedding_vector\n",
    "\n",
    "\n",
    "def findCosineSimilarity(source_representation, test_representation):\n",
    "    a = np.matmul(np.transpose(source_representation), test_representation)\n",
    "    b = np.sum(np.multiply(source_representation, source_representation))\n",
    "    c = np.sum(np.multiply(test_representation, test_representation))\n",
    "    return 1 - (a / (np.sqrt(b) * np.sqrt(c)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create an empty DataFrame with 2623 columns\n",
    "columns = [f\"Column_{i}\" for i in range(1, 2624)]\n",
    "points_of_face = pd.DataFrame(columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_id_temp={}\n",
    "student_id={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 (no detections), 212.7ms\n",
      "Speed: 4.0ms preprocess, 212.7ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 75.0ms\n",
      "Speed: 8.1ms preprocess, 75.0ms inference, 7.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 576ms/step\n",
      "\n",
      "0: 480x640 (no detections), 321.4ms\n",
      "Speed: 6.5ms preprocess, 321.4ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 76.6ms\n",
      "Speed: 3.3ms preprocess, 76.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 392ms/step\n",
      "\n",
      "0: 480x640 1 phone, 203.6ms\n",
      "Speed: 1.0ms preprocess, 203.6ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 76.2ms\n",
      "Speed: 3.1ms preprocess, 76.2ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 383ms/step\n",
      "\n",
      "0: 480x640 1 phone, 231.3ms\n",
      "Speed: 2.5ms preprocess, 231.3ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 76.9ms\n",
      "Speed: 2.0ms preprocess, 76.9ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 383ms/step\n",
      "\n",
      "0: 480x640 1 phone, 206.9ms\n",
      "Speed: 1.9ms preprocess, 206.9ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 68.1ms\n",
      "Speed: 2.0ms preprocess, 68.1ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 374ms/step\n",
      "\n",
      "0: 480x640 1 phone, 202.4ms\n",
      "Speed: 1.9ms preprocess, 202.4ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 70.5ms\n",
      "Speed: 2.0ms preprocess, 70.5ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 364ms/step\n",
      "\n",
      "0: 480x640 1 phone, 245.7ms\n",
      "Speed: 2.2ms preprocess, 245.7ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 79.9ms\n",
      "Speed: 2.0ms preprocess, 79.9ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 357ms/step\n",
      "\n",
      "0: 480x640 1 phone, 201.4ms\n",
      "Speed: 2.3ms preprocess, 201.4ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 75.8ms\n",
      "Speed: 2.1ms preprocess, 75.8ms inference, 0.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 374ms/step\n",
      "\n",
      "0: 480x640 (no detections), 215.3ms\n",
      "Speed: 2.8ms preprocess, 215.3ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 79.0ms\n",
      "Speed: 1.9ms preprocess, 79.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 377ms/step\n",
      "\n",
      "0: 480x640 (no detections), 245.5ms\n",
      "Speed: 4.9ms preprocess, 245.5ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 68.7ms\n",
      "Speed: 2.0ms preprocess, 68.7ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 374ms/step\n",
      "\n",
      "0: 480x640 (no detections), 248.1ms\n",
      "Speed: 1.0ms preprocess, 248.1ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 71.9ms\n",
      "Speed: 1.9ms preprocess, 71.9ms inference, 7.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 363ms/step\n",
      "\n",
      "0: 480x640 (no detections), 259.7ms\n",
      "Speed: 3.9ms preprocess, 259.7ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 76.7ms\n",
      "Speed: 2.0ms preprocess, 76.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 362ms/step\n",
      "\n",
      "0: 480x640 (no detections), 232.5ms\n",
      "Speed: 7.6ms preprocess, 232.5ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 80.0ms\n",
      "Speed: 2.1ms preprocess, 80.0ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 373ms/step\n",
      "\n",
      "0: 480x640 (no detections), 209.2ms\n",
      "Speed: 2.6ms preprocess, 209.2ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 71.2ms\n",
      "Speed: 2.3ms preprocess, 71.2ms inference, 7.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 351ms/step\n",
      "\n",
      "0: 480x640 1 phone, 201.6ms\n",
      "Speed: 2.0ms preprocess, 201.6ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 77.5ms\n",
      "Speed: 2.0ms preprocess, 77.5ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 348ms/step\n",
      "\n",
      "0: 480x640 (no detections), 245.5ms\n",
      "Speed: 5.2ms preprocess, 245.5ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 76.1ms\n",
      "Speed: 1.3ms preprocess, 76.1ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 344ms/step\n",
      "\n",
      "0: 480x640 (no detections), 202.9ms\n",
      "Speed: 2.7ms preprocess, 202.9ms inference, 8.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 78.8ms\n",
      "Speed: 2.2ms preprocess, 78.8ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 390ms/step\n"
     ]
    }
   ],
   "source": [
    "# tracking\n",
    "session_start=int(time.time())\n",
    "while cap.isOpened():\n",
    "    success, frame = cap.read()\n",
    "    if not success:\n",
    "        break\n",
    "\n",
    "    face_recognized=set() # reset the set which contains faces that appear with each frame\n",
    "    removed_ids=[]\n",
    "    ###########################################################################################################################\n",
    "    # Mobile_tracking\n",
    "    result1=mobile_model(frame)\n",
    "\n",
    "    for result in result1[0].boxes.cpu().numpy():\n",
    "            x1,y1,x2,y2=map(int,result.xyxy[0])\n",
    "            cls=names[int(result.cls[0])]\n",
    "            conf=result.conf[0].round(2)\n",
    "            \n",
    "            if conf >0.90 :\n",
    "                mobile=True\n",
    "    \n",
    "     \n",
    "    ###########################################################################################################################\n",
    "    # Run Face detection\n",
    "\n",
    "    results = face_model.track(frame, persist=True, iou=0.5, show=False, tracker=\"bytetrack.yaml\")  \n",
    "    # Update existing persons and track new ones\n",
    "    if results[0].boxes.id is not None:\n",
    "        track_ids = results[0].boxes.id.int().cpu().tolist()\n",
    "        boxes = results[0].boxes.xyxy.cpu()\n",
    "        classes = results[0].boxes.cls.int().cpu().tolist()\n",
    "        confs = results[0].boxes.conf.tolist()\n",
    "        \n",
    "        detected_ids = set()\n",
    "\n",
    "        for box, track_id, cls, conf in zip(boxes,track_ids, classes, confs):\n",
    "            if cls == PERSON_CLASS_INDEX and conf > 0.4:  # Filter by class and confidence\n",
    "                #####################################################################\n",
    "                # eye state\n",
    "                x1, y1, x2, y2 = box.int().tolist()   \n",
    "                ROI = frame[y1:y2, x1:x2]\n",
    "                if ROI.size != 0 :\n",
    "                    ROI = cv2.cvtColor(ROI,cv2.COLOR_BGR2RGB)\n",
    "                    text = detect_eyes_state(ROI)\n",
    "                   \n",
    "                   ###########################################################################\n",
    "                    # face recognition\n",
    "\n",
    "                    emded=Face_Recognition(ROI)\n",
    "                    flag=False # define the flag to determine if the person appears for the first time or not\n",
    "                    id=track_id\n",
    "                    for i in range(points_of_face.shape[0]):\n",
    "                        sim=findCosineSimilarity(emded,points_of_face.iloc[i,:-1])\n",
    "                        if sim > 0.2 :\n",
    "                            id=points_of_face.iloc[i,-1] # if person appear in the past -----> doesn't take new id\n",
    "                            flag=True\n",
    "                       \n",
    "                        \n",
    "                    face_recognized.add(id)   \n",
    "                    if flag == False:\n",
    "                        x=[] # list to collect vector of face_recognition (emded) and id \n",
    "                        for value in emded:\n",
    "                            x.append(value) \n",
    "                        x.append(track_id)\n",
    "                        \n",
    "                        points_of_face.loc[len(points_of_face)] = (x)   # add new person to points_of_face dataframe\n",
    "                    ##################################################################################  \n",
    "                    if id not in time_id:\n",
    "                        time_id[id] = 0\n",
    "                    process_track(text, id)  \n",
    "                    ################################################################################\n",
    "                    # calculate time for each person appear\n",
    "                    \n",
    "                    if len(face_recognized)>0:\n",
    "                        for i in face_recognized:\n",
    "                            if i in student_id_temp.keys(): # if the person appears in the  frame (appears in face_recognized list) and he existed in previous frames ---> then he continues ---> then update last_seen_time only\n",
    "                                student_id_temp[i]['last_seen_time']=int(time.time())\n",
    "                            else: # he appears in the  frame (appears in face_recognized list) and he wasn't exist in previous frames -----> then he appears for the first time\n",
    "                                student_id_temp[i]={\n",
    "                                    'first_seen_time': int(time.time()),\n",
    "                                    'last_seen_time': int(time.time())\n",
    "                                }   \n",
    "\n",
    "                    \n",
    "                    if len(student_id_temp )>0 :\n",
    "                        for j in student_id_temp.keys():  \n",
    "                            if j not in face_recognized:# if the person exists in (student_id_temp) and not exists in (face_recognized) ----> then he is leave session ----> then I can calculate the duration that he appeared in it\n",
    "                                if j not in student_id:\n",
    "                                    student_id[j] = (student_id_temp[j]['last_seen_time']-student_id_temp[j]['first_seen_time'])\n",
    "                                else:\n",
    "                                    student_id[j] += (student_id_temp[j]['last_seen_time']-student_id_temp[j]['first_seen_time'])   \n",
    "\n",
    "                                removed_ids.append(j) \n",
    "\n",
    "                    \n",
    "                    for id in removed_ids: # remove the person that leaves from (student_id_temp) ------> bec if he appears again calculate time correctly by resetting (first_seen_time)\n",
    "                        student_id_temp.pop(id,None) \n",
    "                            \n",
    "                ###########################################################################\n",
    "    \n",
    "    # Show the frame for visualization (optional)\n",
    "    cv2.imshow('Frame', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "    \n",
    "    \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "session_end=int(time.time())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocessing before calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the time for people who exist until the end\n",
    "for j in student_id_temp.keys():\n",
    "    if j not in student_id:\n",
    "        student_id[j] = (student_id_temp[j]['last_seen_time']-student_id_temp[j]['first_seen_time'])\n",
    "    else:\n",
    "        student_id[j] += (student_id_temp[j]['last_seen_time']-student_id_temp[j]['first_seen_time']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 12}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_time_of_session=session_end-session_start # calculate the total time of the session from beginning to end\n",
    "upper_time_limit=total_time_of_session*0.95 # the maximum limit allowed\n",
    "lower_time_limit=total_time_of_session*0.90 # the minimum limit allowed\n",
    "limit_of_another_people=total_time_of_session*0.10 # limit of appearance of another person during the session is 10% of the total session time\n",
    "limit_of_closed_eyes=total_time_of_session*0.15 # limit of closing your eyes is 15% of the total session time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_score=100 # Attention_&_focussing rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the real student -----> who is more people attended\n",
    "student_duration=0\n",
    "for key,value in student_id.items():\n",
    "    if value >= student_duration:\n",
    "        student_duration=value\n",
    "        id_of_real_student=key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total people appear {1: 12}\n"
     ]
    }
   ],
   "source": [
    "print('total people appear',student_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    " # studend_id ----> contain {ID of person , duration of appearance}\n",
    "if (total_time_of_session - student_duration) >= upper_time_limit : # time that the student doesn't exist in session more than 5% of the total session time \n",
    "    total_score-=20\n",
    "if  (total_time_of_session - student_duration) > lower_time_limit: # if the student doesn't attend more than 10% of total session time (try to cover the camera) -----> then he is absent\n",
    "    total_score-=100   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,j in student_id.items():\n",
    "    if( i != id_of_real_student):\n",
    "        if ((j >= limit_of_another_people) and len(student_id) ) > 1: # if there is another person and exists more than the allowd_time(10% of total session time)\n",
    "            total_score-=20\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "if mobile:  # if a mobile phone appears with the student -----> it will decrease the focussing rate by 40%\n",
    "  total_score-=40\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "if time_id[id_of_real_student] > limit_of_closed_eyes: # if the student was sleep during session \n",
    "   total_score-=20   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_score=max(0,total_score)\n",
    "percentage_of_attention=(total_score)/100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "percentage_of_attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if the student holds the mobile phone or not\n",
    "mobile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how many time the student sleeps during the session\n",
    "time_id[id_of_real_student]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how many time the student doesn't attend the session or try to covers the camera\n",
    "(total_time_of_session - student_duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# how many another people exist during the session and how many time they attend\n",
    "count_of_another_people=0\n",
    "time_of_another_people=0\n",
    "\n",
    "\n",
    "for i,j in student_id.items():\n",
    "    if( i != id_of_real_student):\n",
    "       count_of_another_people +=1\n",
    "       time_of_another_people += j\n",
    "\n",
    "\n",
    "print(count_of_another_people)\n",
    "print(time_of_another_people)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mediapipe_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
